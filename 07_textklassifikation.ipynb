{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb470ec0d93c0443",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Textklassifikation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b56e378596dcd5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Datensatz laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a8d1be8983769d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T13:33:14.401723812Z",
     "start_time": "2023-11-28T13:33:14.049346011Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c9324be11c7280",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T13:33:14.558897125Z",
     "start_time": "2023-11-28T13:33:14.064079174Z"
    }
   },
   "outputs": [],
   "source": [
    "data = datasets.fetch_20newsgroups(shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae61667b0f26f564",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T13:33:14.559193881Z",
     "start_time": "2023-11-28T13:33:14.425786446Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ea93390214d64a3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5dcba0af3f3a63",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Aufgabe 1:** Verwende zunächst die ersten 10 Dokumente aus dem Trainings-Datensatz, um Schritt für Schritt eine Funktion `tokenize_documents` aufzubauen.\n",
    "\n",
    "1) Entferne alle Zeilenumbrüche (`\"\\n\"`) und konvertiere jedes Dokument in Kleinbuchstaben.\n",
    "2) Ersetze alle Satzzeichen usw. durch Leerzeichen.\n",
    "3) Splitte jedes Dokument in einzelne Wörter. (Achte darauf, dass die Liste der Wörter keine leeren Einträge `\"\"` enthält.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fcd9aa07ec178f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T13:33:14.563480391Z",
     "start_time": "2023-11-28T13:33:14.425990801Z"
    }
   },
   "outputs": [],
   "source": [
    "docs = x_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7e4ebf2c7c3d7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-28T13:33:14.517375912Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_documents(docs):\n",
    "    tokenized = []\n",
    "    for doc in docs:\n",
    "        # START Dein Code\n",
    "        \n",
    "        ...\n",
    "        \n",
    "        # ENDE\n",
    "        tokenized.append(doc)\n",
    "    \n",
    "    return docs\n",
    "\n",
    "tokenize_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b03ee408d0f7d5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Aufgabe 2:**\n",
    "\n",
    "Welche Wörter kommen im Datensatz vor? Wie oft kommen diese vor?\n",
    "\n",
    "Erstelle dazu ein Dictionary `vocab` dessen Schlüssel die Wörter und dessen Wert jeweils die Anzahl des Vorkommens des Worts ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65404ccaf515f46d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-28T13:33:14.524692766Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_docs = tokenize_documents(x_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd671f55378b61c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-28T13:33:14.569779658Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_vocabulary(tokenized_docs):\n",
    "    vocab = {}\n",
    "    \n",
    "    for doc in tokenized_docs:\n",
    "        # START Dein Code\n",
    "\n",
    "        ...\n",
    "    \n",
    "        # ENDE\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "create_vocabulary(tokenized_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b051f9a039c376",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Aufgabe 3:* Erstelle ein Vokabular für den gesamten Trainings-Datensatz.\n",
    "\n",
    "Wie sind die Wort-Anzahlen verteilt? Plotte z.B. ein Histogramm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c87134ef0377154",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T13:33:14.613968530Z",
     "start_time": "2023-11-28T13:33:14.569886375Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = create_vocabulary(tokenize_documents(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6156537b2644e0f1",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-28T13:33:14.569932261Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309cb934183634ca",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-28T13:33:14.569966134Z"
    }
   },
   "outputs": [],
   "source": [
    "**Aufgabe 4:** Was sind die häufigsten Wörter? In wie viel Prozent der Dokumente kommen diese vor? Sind diese Wörter geeignet, um die verschiedenen Kategorien voneinander zu unterscheiden?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20704b5796e80dec",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-28T13:33:14.570022915Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e18473dc196e7304",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Aufgabe 5:** Schreibe eine Funktion, die die häufigsten Wörter ohne Unterscheidungskraft (sog. \"stopwords\") aus den Dokumenten entfernt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d126a698ab30ec76",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-28T13:33:14.570130331Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_docs = tokenize_documents(x_train[:10])\n",
    "\n",
    "stopwords = ...\n",
    "\n",
    "def remove_stopwords(tokenized_docs):\n",
    "    cleaned_docs = []\n",
    "    for doc in tokenized_docs:\n",
    "        # START Dein Code\n",
    "\n",
    "        ...\n",
    "    \n",
    "        # ENDE\n",
    "        cleaned_docs.append(doc)\n",
    "    \n",
    "    return cleaned_docs\n",
    "\n",
    "remove_stopwords(tokenized_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519c16bc525b83e4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Aufgabe 6:** Erstelle das Vokabular des gesamten Trainings-Datensatzes ohne Stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0d24683cdde45b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-28T13:33:14.570172724Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f45d57596c04b9bd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Aufgabe 7:** Erstelle eine Liste `feature_words` der 1.000 häufigsten Wörter.\n",
    "\n",
    "Erstelle eine Funktion `extract_features(preprocessed_documents, feature_words)` die jedes Dokument eine Liste `bow_doc` der Länge 1000 konvertiert.\n",
    "\n",
    "Jeder Eintrag der Liste entspricht einem Wort aus `feature_words`. Wenn das Wort in dem Dokument vorkommt, ist der Eintrag `1`, ansonsten `0`.\n",
    "\n",
    "Zum Beispiel: Wenn `feature_words[5]` das Wort \"Hund\" ist und \"Hund\" im Dokument vorkommt, dann ist `bow_doc[5] == 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aba7e03caa5481",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-28T13:33:14.570260375Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_words = ...\n",
    "\n",
    "tokenized_docs = remove_stopwords(tokenize_documents(x_train[:10]))\n",
    "\n",
    "def extract_features(tokenized_docs, feature_words):\n",
    "    ...\n",
    "\n",
    "extract_features(tokenized_docs, feature_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b411c6ee26b45f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Aufgabe 8:** Kombiniere die erstellten Funktionen zu einem kompletten Text-Preprocessing:\n",
    "\n",
    "- Test- und Trainingsdaten mit `tokenize_documents` und `remove_stopwords` vorbereiten.\n",
    "- Aus den Trainingsdaten das Vokabular ermitteln (aus Aufgabe 6 kopieren).\n",
    "- Aus dem Vokabular die `feature_words` ermitteln.\n",
    "- Test- und Trainingsdaten mit `extract_features` in Vektoren umwandeln."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202980f9a8fceecf",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-28T13:33:14.570378477Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa432426f1a108bf",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8f71aaaa387bb8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-28T13:33:14.570441334Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3272d0e21fce5399",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Klassifikation mit Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598fbdfc61ad347e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Idee:** \n",
    "\n",
    "Für jedes Wort $x_i$ gibt es eine Wahrscheinlichkeit, dass es in einem Text der Kategorie $y$ auftaucht: $P(x_i | y )$\n",
    "\n",
    "Die Wahrscheinlichkeit, dass ein Text zur Kategorie $y$ gehört, ist dann $P(y | x_1, ..., x_n) = P(x_1 | y) P(x_2 | y) ... = \\prod_i^n P(x_i | y)$\n",
    "\n",
    "In unserem Fall nehmen wir an, dass $P(x_i|y) = \\frac{N_{y,i} + \\alpha}{N_y + \\alpha n}$, wobei $N_{y, i}$ die Anzahl ist, mit der $x_i$ in Texten der Kategorie $y$ auftaucht und $N_y$ die Gesamtzahl aller Worte/Features in der Klasse $y$ (also die Summe aller $N_{y,i}$).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809ee64bb29a4dfc",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-28T13:33:14.570508801Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b23506452329f49",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Term Frequency - Inverse Document Frequency (tf-idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a134ea75f03fba4",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-28T13:33:14.570600852Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcb6807400140de2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Term Frequency:** Anzahl der Vorkommen eines Worts gewichtet nach der Länge des Texts.\n",
    "\n",
    "**Document Frequency:** Worte die in vielen Texten vorkommen werden geringer gewichtet als Worte, die nur in wenigen Texten vorkommen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5b66ca6f2f9305",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-28T13:33:14.613731698Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6acd20b2726354",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T13:34:34.155109276Z",
     "start_time": "2023-11-28T13:34:33.513649941Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.utils._param_validation import InvalidParameterError\n",
    "\n",
    "tr = []\n",
    "ts = []\n",
    "ps = []\n",
    "par = \"max_df\"\n",
    "for i in range(1, 100):\n",
    "    p = 1 - np.log10(i/10)\n",
    "\n",
    "    tf_idf = TfidfVectorizer(max_features=100, **{par: p})\n",
    "    \n",
    "    try:\n",
    "        _x = tf_idf.fit_transform(x_train)\n",
    "    except InvalidParameterError:\n",
    "        continue\n",
    "    _xt = tf_idf.transform(x_test)\n",
    "    \n",
    "    _m = MultinomialNB()\n",
    "    _m.fit(_x, y_train)\n",
    "    ps.append(p)\n",
    "    tr.append(_m.score(_x, y_train))\n",
    "    ts.append(_m.score(_xt, y_test))\n",
    "    \n",
    "plt.plot(ps, tr, label=\"training\")\n",
    "plt.plot(ps, ts, label=\"test\")\n",
    "plt.xlabel(par)\n",
    "plt.ylabel(\"score\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea55d879a015f318",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-28T13:33:14.613928371Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
